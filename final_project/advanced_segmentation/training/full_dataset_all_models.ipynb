{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pytorch_utils import *\n",
    "from preprocess import *\n",
    "from model_container import ModelContainer\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHead(\n",
       "    (0): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50 = ModelContainer(\n",
    "    nnet = get_instrument_segmentation_model_base(\n",
    "        Models.RESNETV50\n",
    "    ),\n",
    "    returns_dict = True\n",
    ")\n",
    "\n",
    "resnet50._network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [64, 64, 48, 64]           9,408\n",
      "       BatchNorm2d-2           [64, 64, 48, 64]             128\n",
      "              ReLU-3           [64, 64, 48, 64]               0\n",
      "         MaxPool2d-4           [64, 64, 24, 32]               0\n",
      "            Conv2d-5           [64, 64, 24, 32]           4,096\n",
      "       BatchNorm2d-6           [64, 64, 24, 32]             128\n",
      "              ReLU-7           [64, 64, 24, 32]               0\n",
      "            Conv2d-8           [64, 64, 24, 32]          36,864\n",
      "       BatchNorm2d-9           [64, 64, 24, 32]             128\n",
      "             ReLU-10           [64, 64, 24, 32]               0\n",
      "           Conv2d-11          [64, 256, 24, 32]          16,384\n",
      "      BatchNorm2d-12          [64, 256, 24, 32]             512\n",
      "           Conv2d-13          [64, 256, 24, 32]          16,384\n",
      "      BatchNorm2d-14          [64, 256, 24, 32]             512\n",
      "             ReLU-15          [64, 256, 24, 32]               0\n",
      "       Bottleneck-16          [64, 256, 24, 32]               0\n",
      "           Conv2d-17           [64, 64, 24, 32]          16,384\n",
      "      BatchNorm2d-18           [64, 64, 24, 32]             128\n",
      "             ReLU-19           [64, 64, 24, 32]               0\n",
      "           Conv2d-20           [64, 64, 24, 32]          36,864\n",
      "      BatchNorm2d-21           [64, 64, 24, 32]             128\n",
      "             ReLU-22           [64, 64, 24, 32]               0\n",
      "           Conv2d-23          [64, 256, 24, 32]          16,384\n",
      "      BatchNorm2d-24          [64, 256, 24, 32]             512\n",
      "             ReLU-25          [64, 256, 24, 32]               0\n",
      "       Bottleneck-26          [64, 256, 24, 32]               0\n",
      "           Conv2d-27           [64, 64, 24, 32]          16,384\n",
      "      BatchNorm2d-28           [64, 64, 24, 32]             128\n",
      "             ReLU-29           [64, 64, 24, 32]               0\n",
      "           Conv2d-30           [64, 64, 24, 32]          36,864\n",
      "      BatchNorm2d-31           [64, 64, 24, 32]             128\n",
      "             ReLU-32           [64, 64, 24, 32]               0\n",
      "           Conv2d-33          [64, 256, 24, 32]          16,384\n",
      "      BatchNorm2d-34          [64, 256, 24, 32]             512\n",
      "             ReLU-35          [64, 256, 24, 32]               0\n",
      "       Bottleneck-36          [64, 256, 24, 32]               0\n",
      "           Conv2d-37          [64, 128, 24, 32]          32,768\n",
      "      BatchNorm2d-38          [64, 128, 24, 32]             256\n",
      "             ReLU-39          [64, 128, 24, 32]               0\n",
      "           Conv2d-40          [64, 128, 12, 16]         147,456\n",
      "      BatchNorm2d-41          [64, 128, 12, 16]             256\n",
      "             ReLU-42          [64, 128, 12, 16]               0\n",
      "           Conv2d-43          [64, 512, 12, 16]          65,536\n",
      "      BatchNorm2d-44          [64, 512, 12, 16]           1,024\n",
      "           Conv2d-45          [64, 512, 12, 16]         131,072\n",
      "      BatchNorm2d-46          [64, 512, 12, 16]           1,024\n",
      "             ReLU-47          [64, 512, 12, 16]               0\n",
      "       Bottleneck-48          [64, 512, 12, 16]               0\n",
      "           Conv2d-49          [64, 128, 12, 16]          65,536\n",
      "      BatchNorm2d-50          [64, 128, 12, 16]             256\n",
      "             ReLU-51          [64, 128, 12, 16]               0\n",
      "           Conv2d-52          [64, 128, 12, 16]         147,456\n",
      "      BatchNorm2d-53          [64, 128, 12, 16]             256\n",
      "             ReLU-54          [64, 128, 12, 16]               0\n",
      "           Conv2d-55          [64, 512, 12, 16]          65,536\n",
      "      BatchNorm2d-56          [64, 512, 12, 16]           1,024\n",
      "             ReLU-57          [64, 512, 12, 16]               0\n",
      "       Bottleneck-58          [64, 512, 12, 16]               0\n",
      "           Conv2d-59          [64, 128, 12, 16]          65,536\n",
      "      BatchNorm2d-60          [64, 128, 12, 16]             256\n",
      "             ReLU-61          [64, 128, 12, 16]               0\n",
      "           Conv2d-62          [64, 128, 12, 16]         147,456\n",
      "      BatchNorm2d-63          [64, 128, 12, 16]             256\n",
      "             ReLU-64          [64, 128, 12, 16]               0\n",
      "           Conv2d-65          [64, 512, 12, 16]          65,536\n",
      "      BatchNorm2d-66          [64, 512, 12, 16]           1,024\n",
      "             ReLU-67          [64, 512, 12, 16]               0\n",
      "       Bottleneck-68          [64, 512, 12, 16]               0\n",
      "           Conv2d-69          [64, 128, 12, 16]          65,536\n",
      "      BatchNorm2d-70          [64, 128, 12, 16]             256\n",
      "             ReLU-71          [64, 128, 12, 16]               0\n",
      "           Conv2d-72          [64, 128, 12, 16]         147,456\n",
      "      BatchNorm2d-73          [64, 128, 12, 16]             256\n",
      "             ReLU-74          [64, 128, 12, 16]               0\n",
      "           Conv2d-75          [64, 512, 12, 16]          65,536\n",
      "      BatchNorm2d-76          [64, 512, 12, 16]           1,024\n",
      "             ReLU-77          [64, 512, 12, 16]               0\n",
      "       Bottleneck-78          [64, 512, 12, 16]               0\n",
      "           Conv2d-79          [64, 256, 12, 16]         131,072\n",
      "      BatchNorm2d-80          [64, 256, 12, 16]             512\n",
      "             ReLU-81          [64, 256, 12, 16]               0\n",
      "           Conv2d-82            [64, 256, 6, 8]         589,824\n",
      "      BatchNorm2d-83            [64, 256, 6, 8]             512\n",
      "             ReLU-84            [64, 256, 6, 8]               0\n",
      "           Conv2d-85           [64, 1024, 6, 8]         262,144\n",
      "      BatchNorm2d-86           [64, 1024, 6, 8]           2,048\n",
      "           Conv2d-87           [64, 1024, 6, 8]         524,288\n",
      "      BatchNorm2d-88           [64, 1024, 6, 8]           2,048\n",
      "             ReLU-89           [64, 1024, 6, 8]               0\n",
      "       Bottleneck-90           [64, 1024, 6, 8]               0\n",
      "           Conv2d-91            [64, 256, 6, 8]         262,144\n",
      "      BatchNorm2d-92            [64, 256, 6, 8]             512\n",
      "             ReLU-93            [64, 256, 6, 8]               0\n",
      "           Conv2d-94            [64, 256, 6, 8]         589,824\n",
      "      BatchNorm2d-95            [64, 256, 6, 8]             512\n",
      "             ReLU-96            [64, 256, 6, 8]               0\n",
      "           Conv2d-97           [64, 1024, 6, 8]         262,144\n",
      "      BatchNorm2d-98           [64, 1024, 6, 8]           2,048\n",
      "             ReLU-99           [64, 1024, 6, 8]               0\n",
      "      Bottleneck-100           [64, 1024, 6, 8]               0\n",
      "          Conv2d-101            [64, 256, 6, 8]         262,144\n",
      "     BatchNorm2d-102            [64, 256, 6, 8]             512\n",
      "            ReLU-103            [64, 256, 6, 8]               0\n",
      "          Conv2d-104            [64, 256, 6, 8]         589,824\n",
      "     BatchNorm2d-105            [64, 256, 6, 8]             512\n",
      "            ReLU-106            [64, 256, 6, 8]               0\n",
      "          Conv2d-107           [64, 1024, 6, 8]         262,144\n",
      "     BatchNorm2d-108           [64, 1024, 6, 8]           2,048\n",
      "            ReLU-109           [64, 1024, 6, 8]               0\n",
      "      Bottleneck-110           [64, 1024, 6, 8]               0\n",
      "          Conv2d-111            [64, 256, 6, 8]         262,144\n",
      "     BatchNorm2d-112            [64, 256, 6, 8]             512\n",
      "            ReLU-113            [64, 256, 6, 8]               0\n",
      "          Conv2d-114            [64, 256, 6, 8]         589,824\n",
      "     BatchNorm2d-115            [64, 256, 6, 8]             512\n",
      "            ReLU-116            [64, 256, 6, 8]               0\n",
      "          Conv2d-117           [64, 1024, 6, 8]         262,144\n",
      "     BatchNorm2d-118           [64, 1024, 6, 8]           2,048\n",
      "            ReLU-119           [64, 1024, 6, 8]               0\n",
      "      Bottleneck-120           [64, 1024, 6, 8]               0\n",
      "          Conv2d-121            [64, 256, 6, 8]         262,144\n",
      "     BatchNorm2d-122            [64, 256, 6, 8]             512\n",
      "            ReLU-123            [64, 256, 6, 8]               0\n",
      "          Conv2d-124            [64, 256, 6, 8]         589,824\n",
      "     BatchNorm2d-125            [64, 256, 6, 8]             512\n",
      "            ReLU-126            [64, 256, 6, 8]               0\n",
      "          Conv2d-127           [64, 1024, 6, 8]         262,144\n",
      "     BatchNorm2d-128           [64, 1024, 6, 8]           2,048\n",
      "            ReLU-129           [64, 1024, 6, 8]               0\n",
      "      Bottleneck-130           [64, 1024, 6, 8]               0\n",
      "          Conv2d-131            [64, 256, 6, 8]         262,144\n",
      "     BatchNorm2d-132            [64, 256, 6, 8]             512\n",
      "            ReLU-133            [64, 256, 6, 8]               0\n",
      "          Conv2d-134            [64, 256, 6, 8]         589,824\n",
      "     BatchNorm2d-135            [64, 256, 6, 8]             512\n",
      "            ReLU-136            [64, 256, 6, 8]               0\n",
      "          Conv2d-137           [64, 1024, 6, 8]         262,144\n",
      "     BatchNorm2d-138           [64, 1024, 6, 8]           2,048\n",
      "            ReLU-139           [64, 1024, 6, 8]               0\n",
      "      Bottleneck-140           [64, 1024, 6, 8]               0\n",
      "          Conv2d-141            [64, 512, 6, 8]         524,288\n",
      "     BatchNorm2d-142            [64, 512, 6, 8]           1,024\n",
      "            ReLU-143            [64, 512, 6, 8]               0\n",
      "          Conv2d-144            [64, 512, 3, 4]       2,359,296\n",
      "     BatchNorm2d-145            [64, 512, 3, 4]           1,024\n",
      "            ReLU-146            [64, 512, 3, 4]               0\n",
      "          Conv2d-147           [64, 2048, 3, 4]       1,048,576\n",
      "     BatchNorm2d-148           [64, 2048, 3, 4]           4,096\n",
      "          Conv2d-149           [64, 2048, 3, 4]       2,097,152\n",
      "     BatchNorm2d-150           [64, 2048, 3, 4]           4,096\n",
      "            ReLU-151           [64, 2048, 3, 4]               0\n",
      "      Bottleneck-152           [64, 2048, 3, 4]               0\n",
      "          Conv2d-153            [64, 512, 3, 4]       1,048,576\n",
      "     BatchNorm2d-154            [64, 512, 3, 4]           1,024\n",
      "            ReLU-155            [64, 512, 3, 4]               0\n",
      "          Conv2d-156            [64, 512, 3, 4]       2,359,296\n",
      "     BatchNorm2d-157            [64, 512, 3, 4]           1,024\n",
      "            ReLU-158            [64, 512, 3, 4]               0\n",
      "          Conv2d-159           [64, 2048, 3, 4]       1,048,576\n",
      "     BatchNorm2d-160           [64, 2048, 3, 4]           4,096\n",
      "            ReLU-161           [64, 2048, 3, 4]               0\n",
      "      Bottleneck-162           [64, 2048, 3, 4]               0\n",
      "          Conv2d-163            [64, 512, 3, 4]       1,048,576\n",
      "     BatchNorm2d-164            [64, 512, 3, 4]           1,024\n",
      "            ReLU-165            [64, 512, 3, 4]               0\n",
      "          Conv2d-166            [64, 512, 3, 4]       2,359,296\n",
      "     BatchNorm2d-167            [64, 512, 3, 4]           1,024\n",
      "            ReLU-168            [64, 512, 3, 4]               0\n",
      "          Conv2d-169           [64, 2048, 3, 4]       1,048,576\n",
      "     BatchNorm2d-170           [64, 2048, 3, 4]           4,096\n",
      "            ReLU-171           [64, 2048, 3, 4]               0\n",
      "      Bottleneck-172           [64, 2048, 3, 4]               0\n",
      "   ResNetEncoder-173  [[-1, 3, 96, 128], [-1, 64, 48, 64], [-1, 256, 24, 32], [-1, 512, 12, 16], [-1, 1024, 6, 8], [-1, 2048, 3, 4]]               0\n",
      "        Identity-174           [64, 2048, 3, 4]               0\n",
      "        Identity-175           [64, 3072, 6, 8]               0\n",
      "       Attention-176           [64, 3072, 6, 8]               0\n",
      "          Conv2d-177            [64, 256, 6, 8]       7,077,888\n",
      "     BatchNorm2d-178            [64, 256, 6, 8]             512\n",
      "            ReLU-179            [64, 256, 6, 8]               0\n",
      "          Conv2d-180            [64, 256, 6, 8]         589,824\n",
      "     BatchNorm2d-181            [64, 256, 6, 8]             512\n",
      "            ReLU-182            [64, 256, 6, 8]               0\n",
      "        Identity-183            [64, 256, 6, 8]               0\n",
      "       Attention-184            [64, 256, 6, 8]               0\n",
      "    DecoderBlock-185            [64, 256, 6, 8]               0\n",
      "        Identity-186          [64, 768, 12, 16]               0\n",
      "       Attention-187          [64, 768, 12, 16]               0\n",
      "          Conv2d-188          [64, 128, 12, 16]         884,736\n",
      "     BatchNorm2d-189          [64, 128, 12, 16]             256\n",
      "            ReLU-190          [64, 128, 12, 16]               0\n",
      "          Conv2d-191          [64, 128, 12, 16]         147,456\n",
      "     BatchNorm2d-192          [64, 128, 12, 16]             256\n",
      "            ReLU-193          [64, 128, 12, 16]               0\n",
      "        Identity-194          [64, 128, 12, 16]               0\n",
      "       Attention-195          [64, 128, 12, 16]               0\n",
      "    DecoderBlock-196          [64, 128, 12, 16]               0\n",
      "        Identity-197          [64, 384, 24, 32]               0\n",
      "       Attention-198          [64, 384, 24, 32]               0\n",
      "          Conv2d-199           [64, 64, 24, 32]         221,184\n",
      "     BatchNorm2d-200           [64, 64, 24, 32]             128\n",
      "            ReLU-201           [64, 64, 24, 32]               0\n",
      "          Conv2d-202           [64, 64, 24, 32]          36,864\n",
      "     BatchNorm2d-203           [64, 64, 24, 32]             128\n",
      "            ReLU-204           [64, 64, 24, 32]               0\n",
      "        Identity-205           [64, 64, 24, 32]               0\n",
      "       Attention-206           [64, 64, 24, 32]               0\n",
      "    DecoderBlock-207           [64, 64, 24, 32]               0\n",
      "        Identity-208          [64, 128, 48, 64]               0\n",
      "       Attention-209          [64, 128, 48, 64]               0\n",
      "          Conv2d-210           [64, 32, 48, 64]          36,864\n",
      "     BatchNorm2d-211           [64, 32, 48, 64]              64\n",
      "            ReLU-212           [64, 32, 48, 64]               0\n",
      "          Conv2d-213           [64, 32, 48, 64]           9,216\n",
      "     BatchNorm2d-214           [64, 32, 48, 64]              64\n",
      "            ReLU-215           [64, 32, 48, 64]               0\n",
      "        Identity-216           [64, 32, 48, 64]               0\n",
      "       Attention-217           [64, 32, 48, 64]               0\n",
      "    DecoderBlock-218           [64, 32, 48, 64]               0\n",
      "          Conv2d-219          [64, 16, 96, 128]           4,608\n",
      "     BatchNorm2d-220          [64, 16, 96, 128]              32\n",
      "            ReLU-221          [64, 16, 96, 128]               0\n",
      "          Conv2d-222          [64, 16, 96, 128]           2,304\n",
      "     BatchNorm2d-223          [64, 16, 96, 128]              32\n",
      "            ReLU-224          [64, 16, 96, 128]               0\n",
      "        Identity-225          [64, 16, 96, 128]               0\n",
      "       Attention-226          [64, 16, 96, 128]               0\n",
      "    DecoderBlock-227          [64, 16, 96, 128]               0\n",
      "     UnetDecoder-228          [64, 16, 96, 128]               0\n",
      "          Conv2d-229           [64, 3, 96, 128]             435\n",
      "        Identity-230           [64, 3, 96, 128]               0\n",
      "        Identity-231           [64, 3, 96, 128]               0\n",
      "      Activation-232           [64, 3, 96, 128]               0\n",
      "================================================================\n",
      "Total params: 32,521,395\n",
      "Trainable params: 32,521,395\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 9.00\n",
      "Forward/backward pass size (MB): 7305.00\n",
      "Params size (MB): 124.06\n",
      "Estimated Total Size (MB): 7438.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "unet = ModelContainer(\n",
    "    nnet = get_instrument_segmentation_model_base(\n",
    "        Models.UNET\n",
    "    ),\n",
    "    returns_dict = False\n",
    ")\n",
    "\n",
    "summary(unet._network, (3, 96, 128), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [64, 32, 94, 126]             896\n",
      "       BatchNorm2d-2          [64, 32, 94, 126]              64\n",
      "              ReLU-3          [64, 32, 94, 126]               0\n",
      "            Conv2d-4          [64, 64, 94, 126]          51,264\n",
      "       BatchNorm2d-5          [64, 64, 94, 126]             128\n",
      "         MaxPool2d-6           [64, 64, 47, 63]               0\n",
      "              ReLU-7           [64, 64, 47, 63]               0\n",
      "            Conv2d-8          [64, 128, 45, 61]         401,536\n",
      "         MaxPool2d-9          [64, 128, 22, 30]               0\n",
      "             ReLU-10          [64, 128, 22, 30]               0\n",
      "           Conv2d-11          [64, 128, 20, 28]         147,584\n",
      "      BatchNorm2d-12          [64, 128, 20, 28]             256\n",
      "             ReLU-13          [64, 128, 20, 28]               0\n",
      "  ConvTranspose2d-14          [64, 128, 22, 30]         147,584\n",
      "      BatchNorm2d-15          [64, 128, 22, 30]             256\n",
      "             ReLU-16          [64, 128, 22, 30]               0\n",
      "  ConvTranspose2d-17           [64, 32, 47, 63]         102,432\n",
      "             ReLU-18           [64, 32, 47, 63]               0\n",
      "  ConvTranspose2d-19           [64, 3, 95, 127]             867\n",
      "         Upsample-20           [64, 3, 96, 128]               0\n",
      "          Softmax-21           [64, 3, 96, 128]               0\n",
      "================================================================\n",
      "Total params: 852,867\n",
      "Trainable params: 852,867\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 9.00\n",
      "Forward/backward pass size (MB): 2109.52\n",
      "Params size (MB): 3.25\n",
      "Estimated Total Size (MB): 2121.77\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thegr\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (96, 128)\n",
    "\n",
    "custom = \\\n",
    "    ModelContainer(\n",
    "        get_instrument_segmentation_model_base(Models.CUSTOM, shape = input_shape),\n",
    "        returns_dict = False\n",
    "    )\n",
    "\n",
    "summary(custom._network, (3, 96, 128), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_name = \"training_full_96_128\"\n",
    "\n",
    "from model_container import _pickle_path\n",
    "\n",
    "if not os.path.exists(\n",
    "    os.path.join(_pickle_path, dataloader_name + \".pkl\")\n",
    "):\n",
    "    input_videos, output_videos = load_endovis_videos(DatasetType.TRAINING)\n",
    "    \n",
    "    # Add extra frame to make training size 4800 in order for batch size to be uniform across iterations\n",
    "    input_videos.extend([input_videos[-1]])\n",
    "    output_videos.extend([output_videos[-1]])\n",
    "    \n",
    "    input_preprocessed = preprocess_source_endovis_images(input_videos, (96, 128))\n",
    "    output_preprocessed = preprocess_endovis_target_images(output_videos, (96, 128))\n",
    "    \n",
    "    resnet50.set_dataset(input_preprocessed, output_preprocessed, 128)\n",
    "    unet.set_dataset(input_preprocessed, output_preprocessed, 128)\n",
    "    custom.set_dataset(input_preprocessed, output_preprocessed, 128)\n",
    "    \n",
    "    resnet50.save_loader(\"training_full_96_128\")\n",
    "else:\n",
    "    resnet50.load_loader(os.path.join(_pickle_path, dataloader_name + \".pkl\"))\n",
    "    unet.load_loader(os.path.join(_pickle_path, dataloader_name + \".pkl\"))\n",
    "    custom.load_loader(os.path.join(_pickle_path, dataloader_name + \".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.set_optimizer(torch.optim.Adam(params = resnet50._network.parameters(), lr = 0.01))\n",
    "resnet50.set_cost(torch.nn.BCEWithLogitsLoss())\n",
    "\n",
    "unet.set_optimizer(torch.optim.Adam(params = unet._network.parameters(), lr = 0.01))\n",
    "unet.set_cost(torch.nn.BCEWithLogitsLoss())\n",
    "\n",
    "custom.set_optimizer(torch.optim.Adam(params = custom._network.parameters(), lr = 0.01))\n",
    "custom.set_cost(torch.nn.BCEWithLogitsLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4480, 3, 96, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50._loader.dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40. Last loss: 0.08604232221841812. Duration: 612.36 sec\n",
      "Epoch: 2/40. Last loss: 0.0487963892519474. Duration: 602.89 sec\n",
      "Epoch: 3/40. Last loss: 0.03425125777721405. Duration: 605.95 sec\n",
      "Epoch: 4/40. Last loss: 0.029312796890735626. Duration: 602.89 sec\n",
      "Epoch: 5/40. Last loss: 0.02830924652516842. Duration: 605.74 sec\n",
      "Epoch: 6/40. Last loss: 0.02677743323147297. Duration: 602.79 sec\n",
      "Epoch: 7/40. Last loss: 0.026832932606339455. Duration: 605.50 sec\n",
      "Epoch: 8/40. Last loss: 0.023519285023212433. Duration: 602.68 sec\n",
      "Epoch: 9/40. Last loss: 0.02155611850321293. Duration: 605.52 sec\n",
      "Epoch: 10/40. Last loss: 0.02344406768679619. Duration: 602.78 sec\n",
      "Epoch: 11/40. Last loss: 0.02306094765663147. Duration: 605.52 sec\n",
      "Epoch: 12/40. Last loss: 0.022571230307221413. Duration: 602.61 sec\n",
      "Epoch: 13/40. Last loss: 0.02113419771194458. Duration: 605.93 sec\n",
      "Epoch: 14/40. Last loss: 0.018908914178609848. Duration: 602.70 sec\n",
      "Epoch: 15/40. Last loss: 0.02124330773949623. Duration: 605.39 sec\n",
      "Epoch: 16/40. Last loss: 0.019894568249583244. Duration: 602.77 sec\n",
      "Epoch: 17/40. Last loss: 0.02027038484811783. Duration: 605.59 sec\n",
      "Epoch: 18/40. Last loss: 0.01785222440958023. Duration: 602.75 sec\n",
      "Epoch: 19/40. Last loss: 0.01958400383591652. Duration: 605.53 sec\n",
      "Epoch: 20/40. Last loss: 0.018564531579613686. Duration: 602.64 sec\n",
      "Epoch: 21/40. Last loss: 0.017079638317227364. Duration: 605.42 sec\n",
      "Epoch: 22/40. Last loss: 0.016778448596596718. Duration: 602.78 sec\n",
      "Epoch: 23/40. Last loss: 0.017005370929837227. Duration: 605.65 sec\n",
      "Epoch: 24/40. Last loss: 0.016332639381289482. Duration: 602.74 sec\n",
      "Epoch: 25/40. Last loss: 0.01673796772956848. Duration: 605.68 sec\n",
      "Epoch: 26/40. Last loss: 0.015342175960540771. Duration: 602.72 sec\n",
      "Epoch: 27/40. Last loss: 0.015375343151390553. Duration: 605.73 sec\n",
      "Epoch: 28/40. Last loss: 0.015435785055160522. Duration: 602.92 sec\n",
      "Epoch: 29/40. Last loss: 0.01497084740549326. Duration: 605.78 sec\n",
      "Epoch: 30/40. Last loss: 0.015838511288166046. Duration: 602.96 sec\n",
      "Epoch: 31/40. Last loss: 0.014151273295283318. Duration: 605.86 sec\n",
      "Epoch: 32/40. Last loss: 0.014696505852043629. Duration: 602.78 sec\n",
      "Epoch: 33/40. Last loss: 0.01454173494130373. Duration: 605.71 sec\n",
      "Epoch: 34/40. Last loss: 0.014629557728767395. Duration: 602.81 sec\n",
      "Epoch: 35/40. Last loss: 0.014890138991177082. Duration: 605.71 sec\n",
      "Epoch: 36/40. Last loss: 0.014063932932913303. Duration: 602.95 sec\n",
      "Epoch: 37/40. Last loss: 0.014635218307375908. Duration: 606.33 sec\n",
      "Epoch: 38/40. Last loss: 0.013577851466834545. Duration: 603.02 sec\n",
      "Epoch: 39/40. Last loss: 0.014330429956316948. Duration: 607.88 sec\n",
      "Epoch: 40/40. Last loss: 0.014632612466812134. Duration: 604.98 sec\n",
      "Epoch: 1/40. Last loss: 0.07947245985269547. Duration: 210.59 sec\n",
      "Epoch: 2/40. Last loss: 0.07072562724351883. Duration: 212.19 sec\n",
      "Epoch: 3/40. Last loss: 0.06608479470014572. Duration: 211.09 sec\n",
      "Epoch: 4/40. Last loss: 0.06589625775814056. Duration: 210.71 sec\n",
      "Epoch: 5/40. Last loss: 0.03892705589532852. Duration: 210.72 sec\n",
      "Epoch: 6/40. Last loss: 0.029305675998330116. Duration: 210.39 sec\n",
      "Epoch: 7/40. Last loss: 0.02093362621963024. Duration: 210.25 sec\n",
      "Epoch: 8/40. Last loss: 0.022575881332159042. Duration: 210.41 sec\n",
      "Epoch: 9/40. Last loss: 0.020875567570328712. Duration: 210.55 sec\n",
      "Epoch: 10/40. Last loss: 0.01720072142779827. Duration: 210.39 sec\n",
      "Epoch: 11/40. Last loss: 0.01711944490671158. Duration: 211.20 sec\n",
      "Epoch: 12/40. Last loss: 0.0167408287525177. Duration: 211.30 sec\n",
      "Epoch: 13/40. Last loss: 0.017403999343514442. Duration: 211.22 sec\n",
      "Epoch: 14/40. Last loss: 0.01685529574751854. Duration: 210.83 sec\n",
      "Epoch: 15/40. Last loss: 0.01372597273439169. Duration: 210.53 sec\n",
      "Epoch: 16/40. Last loss: 0.015075902454555035. Duration: 210.49 sec\n",
      "Epoch: 17/40. Last loss: 0.015728101134300232. Duration: 210.65 sec\n",
      "Epoch: 18/40. Last loss: 0.013719497248530388. Duration: 210.31 sec\n",
      "Epoch: 19/40. Last loss: 0.01287330687046051. Duration: 210.68 sec\n",
      "Epoch: 20/40. Last loss: 0.011730863712728024. Duration: 210.52 sec\n",
      "Epoch: 21/40. Last loss: 0.011150896549224854. Duration: 211.20 sec\n",
      "Epoch: 22/40. Last loss: 0.01130354218184948. Duration: 210.42 sec\n",
      "Epoch: 23/40. Last loss: 0.011466761119663715. Duration: 210.97 sec\n",
      "Epoch: 24/40. Last loss: 0.010701845400035381. Duration: 210.61 sec\n",
      "Epoch: 25/40. Last loss: 0.010239672847092152. Duration: 210.65 sec\n",
      "Epoch: 26/40. Last loss: 0.00996248610317707. Duration: 210.52 sec\n",
      "Epoch: 27/40. Last loss: 0.009354074485599995. Duration: 210.90 sec\n",
      "Epoch: 28/40. Last loss: 0.009898237884044647. Duration: 210.47 sec\n",
      "Epoch: 29/40. Last loss: 0.008559192530810833. Duration: 211.20 sec\n",
      "Epoch: 30/40. Last loss: 0.00779317319393158. Duration: 210.57 sec\n",
      "Epoch: 31/40. Last loss: 0.008358420804142952. Duration: 211.00 sec\n",
      "Epoch: 32/40. Last loss: 0.008184129372239113. Duration: 210.36 sec\n",
      "Epoch: 33/40. Last loss: 0.008072578348219395. Duration: 210.77 sec\n",
      "Epoch: 34/40. Last loss: 0.007782778702676296. Duration: 210.36 sec\n",
      "Epoch: 35/40. Last loss: 0.007624979596585035. Duration: 210.68 sec\n",
      "Epoch: 36/40. Last loss: 0.008115763776004314. Duration: 210.76 sec\n",
      "Epoch: 37/40. Last loss: 0.0070207915268838406. Duration: 210.92 sec\n",
      "Epoch: 38/40. Last loss: 0.006851114798337221. Duration: 210.29 sec\n",
      "Epoch: 39/40. Last loss: 0.0067362105473876. Duration: 211.25 sec\n",
      "Epoch: 40/40. Last loss: 0.006745518185198307. Duration: 210.71 sec\n",
      "Epoch: 1/40. Last loss: 0.5958495736122131. Duration: 145.49 sec\n",
      "Epoch: 2/40. Last loss: 0.5895098447799683. Duration: 141.27 sec\n",
      "Epoch: 3/40. Last loss: 0.5859972834587097. Duration: 141.31 sec\n",
      "Epoch: 4/40. Last loss: 0.5792179107666016. Duration: 141.37 sec\n",
      "Epoch: 5/40. Last loss: 0.5760173797607422. Duration: 141.23 sec\n",
      "Epoch: 6/40. Last loss: 0.5758641958236694. Duration: 141.36 sec\n",
      "Epoch: 7/40. Last loss: 0.5739881992340088. Duration: 141.30 sec\n",
      "Epoch: 8/40. Last loss: 0.5718259811401367. Duration: 141.26 sec\n",
      "Epoch: 9/40. Last loss: 0.5715155601501465. Duration: 141.42 sec\n",
      "Epoch: 10/40. Last loss: 0.5710151195526123. Duration: 141.73 sec\n",
      "Epoch: 11/40. Last loss: 0.5708749890327454. Duration: 141.52 sec\n",
      "Epoch: 12/40. Last loss: 0.5707440376281738. Duration: 141.62 sec\n",
      "Epoch: 13/40. Last loss: 0.5700982213020325. Duration: 141.61 sec\n",
      "Epoch: 14/40. Last loss: 0.570338249206543. Duration: 141.44 sec\n",
      "Epoch: 15/40. Last loss: 0.5702117085456848. Duration: 141.58 sec\n",
      "Epoch: 16/40. Last loss: 0.5703030824661255. Duration: 141.55 sec\n",
      "Epoch: 17/40. Last loss: 0.5699774622917175. Duration: 141.52 sec\n",
      "Epoch: 18/40. Last loss: 0.5698370337486267. Duration: 141.71 sec\n",
      "Epoch: 19/40. Last loss: 0.5699400305747986. Duration: 141.78 sec\n",
      "Epoch: 20/40. Last loss: 0.5698151588439941. Duration: 141.53 sec\n",
      "Epoch: 21/40. Last loss: 0.5696026682853699. Duration: 141.60 sec\n",
      "Epoch: 22/40. Last loss: 0.5701153874397278. Duration: 141.58 sec\n",
      "Epoch: 23/40. Last loss: 0.5693629384040833. Duration: 141.42 sec\n",
      "Epoch: 24/40. Last loss: 0.5698553919792175. Duration: 141.62 sec\n",
      "Epoch: 25/40. Last loss: 0.5699824094772339. Duration: 141.77 sec\n",
      "Epoch: 26/40. Last loss: 0.5699421763420105. Duration: 141.66 sec\n",
      "Epoch: 27/40. Last loss: 0.5693672299385071. Duration: 141.59 sec\n",
      "Epoch: 28/40. Last loss: 0.5693968534469604. Duration: 142.03 sec\n",
      "Epoch: 29/40. Last loss: 0.5695003867149353. Duration: 141.56 sec\n",
      "Epoch: 30/40. Last loss: 0.5695512890815735. Duration: 141.61 sec\n",
      "Epoch: 31/40. Last loss: 0.5691221952438354. Duration: 141.61 sec\n",
      "Epoch: 32/40. Last loss: 0.5691446661949158. Duration: 141.60 sec\n",
      "Epoch: 33/40. Last loss: 0.5691641569137573. Duration: 141.54 sec\n",
      "Epoch: 34/40. Last loss: 0.5692426562309265. Duration: 141.54 sec\n",
      "Epoch: 35/40. Last loss: 0.5691975355148315. Duration: 141.52 sec\n",
      "Epoch: 36/40. Last loss: 0.5691298246383667. Duration: 141.57 sec\n",
      "Epoch: 37/40. Last loss: 0.5690040588378906. Duration: 141.81 sec\n",
      "Epoch: 38/40. Last loss: 0.5686314105987549. Duration: 141.43 sec\n",
      "Epoch: 39/40. Last loss: 0.5692463517189026. Duration: 141.82 sec\n",
      "Epoch: 40/40. Last loss: 0.5689772963523865. Duration: 141.33 sec\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 40\n",
    "\n",
    "resnet50.train(NUM_EPOCHS)\n",
    "unet.train(NUM_EPOCHS)\n",
    "custom.train(NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.save_model(\"resnet50_model_full\")\n",
    "resnet50.save_losses(\"resnet50_losses_full\")\n",
    "\n",
    "unet.save_model(\"unet_model_full\")\n",
    "unet.save_losses(\"unet_losses_full\")\n",
    "\n",
    "custom.save_model(\"custom_model_full\")\n",
    "custom.save_losses(\"custom_losses_full\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
